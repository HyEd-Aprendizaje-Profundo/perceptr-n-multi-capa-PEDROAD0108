{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbfc33a8",
   "metadata": {},
   "source": [
    "# Práctica 1: Perceptrón multicapa.\n",
    "\n",
    "Tu jefe pidió a RH que recolectara datos de desempeño de tus compañeros, los resultados se almacenaron en un csv. El punto critico de estos datos es la satisfacción del empleado, entonces ¿Podremos estimar la satisfacción de los empleados con los datos recabados?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe3db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "\n",
    "df = pd.read_csv('Extended_Employee_Performance_and_Productivity_Data.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67edad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar las columnas numéricas\n",
    "numeric_columns = df.select_dtypes(include=['number']).drop('Employee_ID',axis=1)\n",
    "\n",
    "\n",
    "# Si numeric_columns es un Index, conviértelo a lista\n",
    "cols = list(numeric_columns)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(cols), figsize=(5 * len(cols), 4))\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "    axes[i].hist(df[col], bins=20, color='skyblue', edgecolor='black')\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frecuencia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4265314b",
   "metadata": {},
   "source": [
    "**Problemas**, tenemos distribuciones con picos, esos nos indica categorías. Por otro lado, tenemos variables con \"valles\" en su distribución (distribuciones multimodales) por lo que resultaría óptimo aplicar técnicas de feature engeneering. Por último tenemos distribuciones uniformes, por lo que cada una requeriría un procesamiento indivudual, hagamos la vista gorda e intentemos ajustar un MLP con estos datos, solo estandaricemos nuestros datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40920f8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Implementación de Red:\n",
    "\n",
    "To**memos los datos numéricos como nuestra variable X, y la variable objetivo como ***'Employee_Satisfaction_Score'***.\n",
    "- **Actividad 1**: Para todos los strings ``'@modif@'`` que aparescan en el siguiente bloque de código cámbialos para que el código funcione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd081ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = '@modif@'.drop('Employee_Satisfaction_Score',axis = 1)\n",
    "y = numeric_columns.'@modif@'\n",
    "y = y.apply(lambda x: round(x)-1) #Cambiamos la variable objetivo a 5 categorías numéricas\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_standar = scaler.'@modif@'(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = '@modif@'(X_standar, y, test_size=0.33, random_state=42)\n",
    "\n",
    "y_onehot_train = tf.keras.utils.to_categorical(y_train, 5)\n",
    "y_onehot_test = tf.keras.utils.to_categorical(y_test,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5920d085",
   "metadata": {},
   "source": [
    "- **Actividad 2:** Implementa 3 arquitecturas de MLP, cada una con su propio nombre, cambiando la estructura de dichas arquitecturas (capas, neuronas por capa, función de activación, etc). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08db50b",
   "metadata": {},
   "source": [
    "# Arquitectura 1: Simple\n",
    "model1 = models.Sequential(name=\"MLP_simple\")\n",
    "model1.add(layers.Dense(32, activation='relu', input_shape=(X_standar.shape[1],)))\n",
    "model1.add(layers.Dense(5, activation='softmax'))  # 5 categorías\n",
    "\n",
    "# Arquitectura 2: Más profunda\n",
    "model2 = models.Sequential(name=\"MLP_medio\")\n",
    "model2.add(layers.Dense(64, activation='relu', input_shape=(X_standar.shape[1],)))\n",
    "model2.add(layers.Dense(32, activation='relu'))\n",
    "model2.add(layers.Dense(16, activation='relu'))\n",
    "model2.add(layers.Dense(5, activation='softmax'))\n",
    "\n",
    "# Arquitectura 3: Con dropout (para regularizar)\n",
    "model3 = models.Sequential(name=\"MLP_dropout\")\n",
    "model3.add(layers.Dense(128, activation='relu', input_shape=(X_standar.shape[1],)))\n",
    "model3.add(layers.Dropout(0.3))\n",
    "model3.add(layers.Dense(64, activation='relu'))\n",
    "model3.add(layers.Dropout(0.3))\n",
    "model3.add(layers.Dense(5, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b8c585",
   "metadata": {},
   "source": [
    "- **Actividad 3:** Compila y ajusta tus tres modelos con sus respectivos hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9f4a40-1d14-4b2b-8751-01056ee47882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Compilación ===\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# === Entrenamiento ===\n",
    "history1 = model1.fit(X_train, y_onehot_train, epochs=20, batch_size=32,\n",
    "                      validation_data=(X_test, y_onehot_test), verbose=1)\n",
    "\n",
    "history2 = model2.fit(X_train, y_onehot_train, epochs=20, batch_size=32,\n",
    "                      validation_data=(X_test, y_onehot_test), verbose=1)\n",
    "\n",
    "history3 = model3.fit(X_train, y_onehot_train, epochs=20, batch_size=32,\n",
    "                      validation_data=(X_test, y_onehot_test), verbose=1)\n",
    "\n",
    "# === Evaluación ===\n",
    "print(\"\\nEvaluación modelo 1:\")\n",
    "model1.evaluate(X_test, y_onehot_test)\n",
    "\n",
    "print(\"\\nEvaluación modelo 2:\")\n",
    "model2.evaluate(X_test, y_onehot_test)\n",
    "\n",
    "print(\"\\nEvaluación modelo 3:\")\n",
    "model3.evaluate(X_test, y_onehot_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92ff84b",
   "metadata": {},
   "source": [
    "- **Actividad 4:** Sube tus cambios al repositorio, envía el link de tu repositorio a la actividad 2 de tu checkpoint 2 y contesta las preguntas de dicha actividad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7daf499-f478-4773-bc8c-f7733f059f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
